version: "3.5"

services:

  # whisper_service_patch_source_ubuntu_cuda118:
  #   image: "whisper_service_patch_source_ubuntu_cuda118:latest"
  #   build:
  #     context: .
  #     dockerfile: Dockerfiles/whisperx-patch.source.ubuntu_2004_lts.base.cuda_118.dockerfile
  #   volumes:
  #     - ./whisperx:/workspace
  #     # runtime: nvidia  # Enable NVIDIA GPU support
  #     # devices:
  #     # - /dev/nvidiactl
  #     # - /dev/nvidia-uvm
  #   environment:
  #     - CUDA_HOME=/usr/local/cuda-11.8
  #     - PYTORCH_NO_CUDA_MEMORY_CACHING=1
  #     # - M_OPENASSISTANT_PORT=12353
  #     - NVIDIA_VISIBLE_DEVICES=all
  #     - NVIDIA_DRIVER_CAPABILITIES=all
  #     - LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH
  #   ports:
  #     - "55580:80"
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             # count: all
  #             device_ids: [ all ]
  #             capabilities: [ gpu ]

  whisper_service_patch_source_ubuntu_cuda118_dev:
    image: "whisper_service_patch_source_ubuntu_cuda118_dev:latest"
    build:
      context: .
      dockerfile: Dockerfiles/whisperx-patch.source.ubuntu_2004_lts.base.cuda_118.dockerfile
    volumes:
      - ./source:/workspace
      # runtime: nvidia  # Enable NVIDIA GPU support
      # devices:
      # - /dev/nvidiactl
      # - /dev/nvidia-uvm
    environment:
      - CUDA_HOME=/usr/local/cuda-11.8
      - PYTORCH_NO_CUDA_MEMORY_CACHING=1
      # - M_OPENASSISTANT_PORT=12353
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
      # - LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH
      - CONDA_DIR_PATH=/usr/local/miniconda
      # - PATH=/root/miniconda:$PATH
    ports:
      - "55579:80"
      - "8808:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # count: all
              device_ids: [ all ]
              capabilities: [ gpu ]
  # whisper_service_patch_source_nvidia:
  #   image: "whisper_service_patch_source_nvidia:latest"
  #   build:
  #     context: .
  #     dockerfile: whisperx-patch.source.nvidia.dockerfile
  #   volumes:
  #     - ./whisperx:/workspace
  #   # runtime: nvidia  # Enable NVIDIA GPU support
  #   # devices:
  #     # - /dev/nvidiactl
  #     # - /dev/nvidia-uvm
  #   environment:
  #     - CUDA_HOME=/usr/local/cuda-12.2
  #     - PYTORCH_NO_CUDA_MEMORY_CACHING=1
  #     - M_OPENASSISTANT_PORT=12353
  #     - NVIDIA_VISIBLE_DEVICES=all
  #     - NVIDIA_DRIVER_CAPABILITIES=compute
  #   ports:
  #     - "55555:80"

  # whisper_service_patch_source_pytorch:
  #   image: "whisper_service_patch_source_pytorch:latest"
  #   build:
  #     context: .
  #     dockerfile: whisperx-patch.source.pytorch.dockerfile
  #   volumes:
  #     - ./whisperx:/workspace
  #   # runtime: nvidia  # Enable NVIDIA GPU support
  #   # devices:
  #     # - /dev/nvidiactl
  #     # - /dev/nvidia-uvm
  #   environment:
  #     - CUDA_HOME=/usr/local/cuda-12.2
  #     - PYTORCH_NO_CUDA_MEMORY_CACHING=1
  #     - M_OPENASSISTANT_PORT=12353
  #     - NVIDIA_VISIBLE_DEVICES=all
  #     - NVIDIA_DRIVER_CAPABILITIES=compute
  #   ports:
  #     - "55560:80"

  # whisper_service_patch_source_pytorch_cuda118:
  #   image: "whisper_service_patch_source_pytorch_cuda118:latest"
  #   build:
  #     context: .
  #     dockerfile: whisperx-patch.source.pytorch.cuda_118.dockerfile
  #   volumes:
  #     - ./whisperx:/workspace
  #     # runtime: nvidia  # Enable NVIDIA GPU support
  #     # devices:
  #     # - /dev/nvidiactl
  #     # - /dev/nvidia-uvm
  #   environment:
  #     - CUDA_HOME=/usr/local/cuda-11.8
  #     - PYTORCH_NO_CUDA_MEMORY_CACHING=1
  #     # - M_OPENASSISTANT_PORT=12353
  #     - NVIDIA_VISIBLE_DEVICES=all
  #     - NVIDIA_DRIVER_CAPABILITIES=all
  #     - LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH
  #   ports:
  #     - "55560:80"
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             # count: all
  #             device_ids: [ all ]
  #             capabilities: [ gpu ]

  # whisper_service_patch_source_pytorch_cuda118-test:
  #   image: "whisper_service_patch_source_pytorch_cuda118-test:latest"
  #   build:
  #     context: .
  #     dockerfile: whisperx-patch.source.pytorch.cuda_118.dockerfile
  #   volumes:
  #     - ./whisperx:/workspace
  #     # runtime: nvidia  # Enable NVIDIA GPU support
  #     # devices:
  #     # - /dev/nvidiactl
  #     # - /dev/nvidia-uvm
  #   environment:
  #     - CUDA_HOME=/usr/local/cuda-11.8
  #     - PYTORCH_NO_CUDA_MEMORY_CACHING=1
  #     # - M_OPENASSISTANT_PORT=12353
  #     - NVIDIA_VISIBLE_DEVICES=all
  #     - NVIDIA_DRIVER_CAPABILITIES=all
  #     - LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH
  #   ports:
  #     - "55561:80"
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             # count: all
  #             device_ids: [ all ]
  #             capabilities: [ gpu ]

