version: "3.5"

services:
  dev_nvidia_cuda_118:
    image: "dev_nvidia_cuda_118:latest"
    container_name: "dev_nvidia_cuda_118_container"
    build:
      context: .
      dockerfile: Dockerfiles/dev/nvidia/base/cuda_118/ubuntu/ubuntu_2004.cuda_118.dockerfile
    volumes:
      - ./scripts:/scripts:rw
      - ./installer:/installer:rw
      # - ./source:/workspace
      # runtime: nvidia  # Enable NVIDIA GPU support
      # devices:
      # - /dev/nvidiactl
      # - /dev/nvidia-uvm
    environment:
      - CUDA_HOME=/usr/local/cuda-11.8
      - PYTORCH_NO_CUDA_MEMORY_CACHING=1
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
      - CONDA_DIR_PATH=/usr/local/miniconda
      # - M_OPENASSISTANT_PORT=12353
      # - LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH
      # - PATH=/root/miniconda:$PATH
    ports:
      - "55579:80"
      - "8808:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # count: all
              device_ids: [ all ]
              capabilities: [ gpu ]

  dev_nvidia_cuda_118_cudnn:
    image: "dev_nvidia_cuda_118_cudnn:latest"
    container_name: "dev_nvidia_cuda_118_cudnn_container"
    build:
      context: .
      dockerfile: Dockerfiles/dev/nvidia/base/cuda_118/ubuntu/ubuntu_2004.cuda_118.cudnn.dockerfile
    volumes:
      - ./scripts:/scripts:rw
      - ./installer:/installer:rw
      # - ./source:/workspace
      # runtime: nvidia  # Enable NVIDIA GPU support
      # devices:
      # - /dev/nvidiactl
      # - /dev/nvidia-uvm
    environment:
      - CUDA_HOME=/usr/local/cuda-11.8
      - PYTORCH_NO_CUDA_MEMORY_CACHING=1
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
      - CONDA_DIR_PATH=/usr/local/miniconda
      # - M_OPENASSISTANT_PORT=12353
      # - LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH
      # - PATH=/root/miniconda:$PATH
    ports:
      - "55580:80"
      - "8809:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # count: all
              device_ids: [ all ]
              capabilities: [ gpu ]

  dev_nvidia_cuda_118_cudnn_testing:
    image: "dev_nvidia_cuda_118_cudnn:latest"
    container_name: "dev_nvidia_cuda_118_cudnn_test_container"
    build:
      context: .
      dockerfile: Dockerfiles/dev/nvidia/base/cuda_118/ubuntu/ubuntu_2004.cuda_118.cudnn.dockerfile
    volumes:
      - ./scripts:/scripts:rw
      - ./installer:/installer:rw
      # - ./source:/workspace
      # runtime: nvidia  # Enable NVIDIA GPU support
      # devices:
      # - /dev/nvidiactl
      # - /dev/nvidia-uvm
    environment:
      - CUDA_HOME=/usr/local/cuda-11.8
      - PYTORCH_NO_CUDA_MEMORY_CACHING=1
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
      - CONDA_DIR_PATH=/usr/local/miniconda
      # - M_OPENASSISTANT_PORT=12353
      # - LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH
      # - PATH=/root/miniconda:$PATH
    ports:
      - "55521:80"
      - "8810:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # count: all
              device_ids: [ all ]
              capabilities: [ gpu ]
  
  dev_nvidia_cuda_118_cudnn_conda:
    image: "dev_nvidia_cuda_118_cudnn_miniconda3:20.04"
    container_name: "dev_nvidia_cuda_118_cudnn_miniconda3_container"
    build:
      context: .
      dockerfile: Dockerfiles/dev/conda/ubuntu_2004.cuda_118.cudnn.miniconda3.dockerfile
    volumes:
      - ./scripts:/scripts:rw
      - ./installer:/installer:rw
      # - ./source:/workspace
      # runtime: nvidia  # Enable NVIDIA GPU support
      # devices:
      # - /dev/nvidiactl
      # - /dev/nvidia-uvm
    environment:
      - CUDA_HOME=/usr/local/cuda-11.8
      - PYTORCH_NO_CUDA_MEMORY_CACHING=1
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
      - CONDA_DIR_PATH=/usr/local/miniconda
      # - M_OPENASSISTANT_PORT=12353
      # - LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH
      # - PATH=/root/miniconda:$PATH
    ports:
      - "55522:80"
      - "8811:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # count: all
              device_ids: [ all ]
              capabilities: [ gpu ]

  dev_nvidia_cuda_118_cudnn_pytorch:
    image: "dev_nvidia_cuda_118_cudnn_pytorch_v1.13.1:20.04"
    container_name: "dev_nvidia_cuda_118_cudnn_pytorch_v1.13.1_container"
    build:
      context: .
      dockerfile: Dockerfiles/dev/pytorch/ubuntu/ubuntu_2004.cuda_118.cudnn.miniconda3.pytorch_v1.13.1.dockerfile
    volumes:
      - ./scripts:/scripts:rw
      - ./installer:/installer:rw
      - ./patches:/patches:ro
      # - ./source:/workspace
      # runtime: nvidia  # Enable NVIDIA GPU support
      # devices:
      # - /dev/nvidiactl
      # - /dev/nvidia-uvm
    environment:
      - CUDA_HOME=/usr/local/cuda-11.8
      - PYTORCH_NO_CUDA_MEMORY_CACHING=1
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=all
      - CONDA_DIR_PATH=/usr/local/miniconda
      # - M_OPENASSISTANT_PORT=12353
      # - LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH
      # - PATH=/root/miniconda:$PATH
    ports:
      - "55523:80"
      - "8812:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # count: all
              device_ids: [ all ]
              capabilities: [ gpu ]